# RAG App Test â€” Local Retrieval Augmented Generation Chatbot

- A fully local, offline-capable RAG (Retrieval-Augmented Generation) application using:
- Flask (REST API backend)
- ChromaDB (vector database)
- LangChain (RAG pipeline)
- Ollama (local LLM runtime)
- HuggingFace embeddings
- HTML/CSS/JS front-end

This app allows you to upload manuals or text documents, embed them into Chroma, and query them via a chatbot-like web UI using local LLMs.

## âš¡ Features

âœ” 100% local â€” no cloud, no API keys, no cost
âœ” Uses Ollama to run Llama/Mistral/Phi models locally
âœ” Provides semantic search using Chroma vector database
âœ” Modern browser UI with chat bubbles
âœ” â€œBot is typingâ€¦â€ animation
âœ” Easy to run on any Windows/macOS/Linux system
âœ” Highly extensible, beginner-friendly architecture

## ğŸ“ Project Structure
RagAppTest/
â”‚
â”œâ”€â”€ app.py                     # Flask backend (RAG API)
â”œâ”€â”€ ingest.py                  # Embedding & indexing pipeline
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ data/                      # Your documents (manuals, text files)
â”‚     â”œâ”€â”€ manual1.txt
â”‚     â”œâ”€â”€ manual2.txt
â”‚
â”œâ”€â”€ chroma_store/              # Auto-generated vector DB (after ingest)
â”‚
â”œâ”€â”€ static_chat/               # Front-end UI
â”‚     â”œâ”€â”€ index.html
â”‚     â”œâ”€â”€ style.css
â”‚     â”œâ”€â”€ app.js
â”‚
â”œâ”€â”€ test_chat.py               # (Optional) simple local testing script
â””â”€â”€ test_chat_interactive.py   # (Optional) console chatbot

## ğŸ§© Requirements

To run this project locally, you need:

# 1ï¸âƒ£ Python

Download from:
https://www.python.org/downloads/

Version recommended: Python 3.10 â€“ 3.12

# 2ï¸âƒ£ Ollama (Local LLM Engine)

Download from:
https://ollama.com/download

After installation, open a terminal and pull a model:

ollama pull llama3

Verify Ollama is running:

http://127.0.0.1:11434

# 3ï¸âƒ£ Git (optional, for cloning)

https://git-scm.com/downloads

## ğŸš€ Setup Instructions (Works on Any Local PC)

Follow these steps on ANY compututer (Windows/macOS/Linux):

# 1ï¸âƒ£ Clone the repository
git clone https://github.com/janevsk1/RagAppTest.git
cd RagAppTest

Or download ZIP from GitHub.

# 2ï¸âƒ£ Create a virtual environment
Windows:
python -m venv venv
venv\Scripts\activate

macOS/Linux:
python3 -m venv venv
source venv/bin/activate

# 3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

This installs:

- Flask
- ChromaDB
- LangChain
- LangChain-HuggingFace
- LangChain-Ollama
- HuggingFace Hub
- Streamlit (optional)

# 4ï¸âƒ£ Add your documents

Place your .txt manuals or text documents into:

/data

Example:

data/manual1.txt
data/manual2.txt

# 5ï¸âƒ£ Run ingestion (build vector database)
python ingest.py

This will:

- Load your text files
- Chunk them
- Create embeddings
- Store vectors into chroma_store/

You should see:

âœ… Ingested X chunks from Y docs into chroma_store

# 6ï¸âƒ£ Start the backend (Flask API)
python app.py

if you have this error: ImportError: cannot import name 'CORS' from 'flask_cors'

pip install flask-cors

now is added into requirements and will be tested: flask-cors==4.0.0

You should see:

## ğŸš€ RAG Chatbot API running on http://127.0.0.1:5000/chat

Keep this terminal open â€” your backend must stay running.

# 7ï¸âƒ£ Start the Chat UI (Browser)

Navigate to static_chat/:

cd static_chat
python -m http.server 8000


Open browser:
## ğŸ‘‰ http://localhost:8000/index.html

You now have a modern web-based chatbot UI connected to your local RAG engine!

## ğŸ§  Architecture Overview
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   User / UI   â”‚
               â”‚ index.html     â”‚
               â”‚ app.js         â”‚
               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ (POST /chat)
                      â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚       Flask        â”‚
             â”‚     app.py         â”‚
             â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚  RAG Pipeline      â”‚
             â”‚ RetrievalQA        â”‚
             â”‚ LangChain          â”‚
             â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚        Vector Store          â”‚
     â”‚         ChromaDB             â”‚
     â”‚ (semantic search: top K)     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚        Local LLM            â”‚
      â”‚     Ollama (Llama3)         â”‚
      â”‚      http://localhost:11434 â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ›  Troubleshooting
âŒ UI gives â€œFailed to fetchâ€

Cause: Opening index.html with file:// instead of HTTP

Fix:

python -m http.server 8000

âŒ CORS error

Install:

pip install flask-cors


Add to app.py:

from flask_cors import CORS
CORS(app)

âŒ Ollama port already in use
Error: bind: Only one usage of each socket address


Fix:

tasklist | findstr ollama
taskkill /PID xxx /F

ğŸŒŸ Future Enhancements (coming soon)

File upload from UI â†’ automatic re-index

Chat history persistence

Multiple collections in Chroma

LLM selection dropdown (Llama/Mistral/Phi)

Dark mode toggle

Typing speed simulation

Built-in logging panel

ğŸ‘ License

MIT â€” free for personal and commercial use.